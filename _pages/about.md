---
permalink: /
title: "Haotong Yang （杨昊桐）"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---



Email: haotongyang "at" pku "dot" edu "dot" cn

Biography
======
Haotong is a fourth-year Ph.D. student in the [School of Intelligence Science and Technology](https://www.cis.pku.edu.cn/) and the [Institute for Artifical Intelligence](https://www.ai.pku.edu.cn/) at Peking University. He is a member of [ZeroLab](https://zero-lab-pku.github.io/) led by Prof. [Zhouchen Lin](https://zhouchenlin.github.io/) and [GraphPKU](https://www.graphpku.cn/) led by Prof. [Muhan Zhang](https://muhanzhang.github.io/). His research interest lays in **Large Language Model** (**LLM**) especially in reasoning ability of LLMs, explanation and interpretation of LLMs and also multi-modal LLM. Before his graduate stage, he receive a bachelor degree from the [School of Mathematical Science](https://www.math.pku.edu.cn/index.htm) in Peking University, where he worked with Prof. [Zhanxing Zhu](https://zhanxingzhu.github.io/).

杨昊桐，北京大学智能学院和人工智能研究院和在读博士研究生，师从林宙辰教授和张牧涵助理教授，目前主要从事和大语言模型推理、可解释性和多模态大模型相关的研究。在ICLR、ICML、NeurIPS等学术会议上发表多篇文章，并多次担任审稿人。其中一作文章 [Rethinking knowledge graph evaluation under the open-world assumption](https://proceedings.neurips.cc/paper_files/paper/2022/hash/378226e5df7eded3e401de5c9493143c-Abstract-Conference.html) 获评NeurIPS 2022口头报告。本科毕业于北京大学数学科学学院。曾多次获得北京大学三好学生和校级奖学金，并于2021年荣获北京市优秀毕业生。在学术生活之外，他还热心学生工作和社会服务，研究生期间两次担任本科带班辅导员，曾在建国70周年和建党百年纪念活动中担任志愿者。

Research Interest
======
**Reasoning of LLMs**
I focus on explain and the mechanism of reasoning ability of LLMs and improve it. Specifically, how the LLMs can learn the world knowledge (or basic knowledge), learn the concepts and solution of tasks, and solve complex tasks in language reasoning. The topic is related to (1) the mechanism of attention, positional embedding, tokenizer and its relation with various reasoning tasks, (2) various types of chain-of-thought (3) analysis on typical reasoning tasks.

**Multi-modal Reasoning**
I also interest on extending the reasoning framework and capacity of LLMs to multi-modal scenario, such as merge the graph learning by GNN with LLM's powerful factual knowledge learning and reasoning ability or vision-language model's reasoning pipeline.


Publications
======
1. [ICLR-25] **H. Yang**, Y. Hu, S. Kang, Z. Lin, and M. Zhang: **Number Cookbook: Number Understanding of Language Models and How to Improve It**. The Thirteenth International Conference on Learning Representations (ICLR-2025), 2024. ([PDF](https://arxiv.org/abs/2411.03766))
2. [ICML-24] Y. Hu, X. Tang, **Y. Yang**, and M. Zhang: **Case-based or rule-based: how do transformers do the math?** The Forty-First International Conference on Machine Learning (ICML-2024), 2024. ([PDF](https://arxiv.org/abs/2402.17709))
3. [ICLR-24] X. Wang, **H.Yang**, Z. Lin, and M. Zhang: **Neural common neighbor with completion for link prediction**. The Twelfth International Conference on Learning Representations (ICLR-2024), 2023. ([pdf](https://arxiv.org/abs/2302.00890))
4. [NeurIPS-22] **H. Yang**, Z. Lin, and M. Zhang: **Rethinking knowledge graph evaluation under the open-world assumption**. The Thirty-Sixth Annual Conference on Neural Information Processing Systems (NeurIPS-2022), 2022. ([PDF](https://proceedings.neurips.cc/paper_files/paper/2022/file/378226e5df7eded3e401de5c9493143c-Paper-Conference.pdf))([report](https://news.pku.edu.cn/jxky/6d9e681f68fd448c9c323188baed7699.htm)) <font color='red'>oral presentation (1.7% acceptance rate)</font>

Preprint or Under Review
======
1. **H. Yang**\*, Q. Zheng\*, Y. Gao\*, Y. Yang, Y. He, Z. Lin, and M. Zhang: **VACT: A Video Automatic Causal Testing System and a Benchmark** (preprint soon)
1. Y. Hu\*, S. Kang\*, **H. Yang**, H. Xu, and M. Zhang: **Training Large Language Models to be Better Rule Followers** ([arxiv](https://arxiv.org/abs/2502.11525))
1. **H. Yang**\*, Xiyuan Wang\*, Q. Tao, S. Hu, Z. Lin, M. Zhang: **GL-Fusion: Rethinking the Combination of Graph Neural Network and Large Language model** ([arxiv](https://arxiv.org/abs/2412.06849))
1. **H. Yang**, F. Meng, Z. Lin, and M. Zhang: **Parrot Mind: Towards Explaining the Complex Task Reasoning of Pretrained Large Language Models with Template-Content Structure** ([arxiv](https://arxiv.org/abs/2310.05452))
2. Y. Hu, **H. Yang**, Z. Lin and M. Zhang: **Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models** ([arxiv](https://arxiv.org/abs/2305.18507)) 

